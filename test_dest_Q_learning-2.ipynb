{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 200/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 300/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 400/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 500/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 600/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 700/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 800/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 900/1000, Reward: -1.8369701987210297e-16\n",
      "Episode 1000/1000, Reward: -1.8369701987210297e-16\n",
      "Training complete.\n",
      "Final Q-table:\n",
      "[[[-1.07383553e+00 -1.03902092e-01 -8.79775665e-01]\n",
      "  [-1.96599595e+00 -1.83907501e+00 -1.33296595e+00]\n",
      "  [-7.32455449e-01 -1.90879683e+00 -6.31284365e-01]\n",
      "  [-5.80859749e-01 -2.73254241e-01 -1.36776711e-02]\n",
      "  [-1.08758387e+00 -1.37698212e+00 -7.65619409e-02]\n",
      "  [-1.82147000e+00 -5.06613582e-01 -1.95780735e+00]\n",
      "  [-4.75534790e-01 -6.64356370e-02 -1.17881811e-02]\n",
      "  [-1.67323887e+00 -1.80942689e+00 -9.31844381e-01]\n",
      "  [-1.61376664e-01 -1.17437777e+00 -4.35036897e-01]\n",
      "  [-6.00672023e-01 -4.12839707e-02 -2.17341581e-01]]\n",
      "\n",
      " [[-2.23929091e-01 -1.16711750e+00 -1.12587786e+00]\n",
      "  [-1.52263451e+00 -1.33182703e-02 -6.67020588e-01]\n",
      "  [-8.25930805e-01 -1.43830202e+00 -8.02809277e-01]\n",
      "  [-7.05719360e-01 -1.10613145e+00 -3.90510967e-01]\n",
      "  [-1.82818610e+00 -1.64724883e+00 -5.63967767e-01]\n",
      "  [-9.61534846e-01 -5.80173681e-01 -3.87828229e-03]\n",
      "  [-6.10229423e-01 -1.45299591e-01 -4.22936425e-03]\n",
      "  [-5.61844666e-01 -6.69794558e-01 -3.72889964e-01]\n",
      "  [-1.71594274e+00 -3.10437165e-01 -8.85467147e-01]\n",
      "  [-4.46097735e-01 -1.63268329e+00 -1.48833170e+00]]\n",
      "\n",
      " [[-5.95505015e-01 -4.73822886e-02 -1.54740744e+00]\n",
      "  [-1.96940370e+00 -1.66835586e+00 -6.06307108e-01]\n",
      "  [-1.67904476e-01 -1.34482728e+00 -5.60815651e-01]\n",
      "  [-5.76296396e-02 -5.20033026e-01 -1.87440950e+00]\n",
      "  [-1.70712383e+00 -1.22246567e+00 -9.99427513e-01]\n",
      "  [-9.03086901e-01 -7.68627364e-02 -2.33179747e-02]\n",
      "  [-1.60318597e+00 -3.36373666e-01 -4.91152289e-01]\n",
      "  [-1.64442818e+00 -8.74765789e-01 -1.34072410e+00]\n",
      "  [-1.80674090e+00 -2.07501011e-01 -1.64313999e+00]\n",
      "  [-9.27125287e-01 -6.83049422e-01 -1.12114083e+00]]\n",
      "\n",
      " [[-1.71417641e+00 -3.24800163e-01 -1.44258625e+00]\n",
      "  [-6.87809851e-01 -2.40435788e-01 -1.99270872e+00]\n",
      "  [-7.94026480e-01 -1.65913782e+00 -6.56501850e-01]\n",
      "  [-1.72547342e+00 -1.31397880e+00 -7.16161886e-01]\n",
      "  [-1.04291568e+00 -3.75109611e-01 -7.30351079e-01]\n",
      "  [-1.64808275e-01 -2.49320685e-01 -7.71984587e-01]\n",
      "  [-1.46295371e+00 -1.05341731e+00 -8.27307329e-01]\n",
      "  [-5.77496915e-01 -3.03793645e-02 -1.08732874e+00]\n",
      "  [-1.92343329e+00 -1.93959369e+00 -1.56476356e+00]\n",
      "  [-3.17273292e-01 -1.92595000e+00 -1.45270617e+00]]\n",
      "\n",
      " [[-3.73830306e-01 -1.28948684e+00 -1.66827423e+00]\n",
      "  [-5.74281670e-01 -1.68409481e+00 -8.41039295e-01]\n",
      "  [-5.05367577e-01 -1.95155066e+00 -1.05090673e+00]\n",
      "  [-4.60219189e-01 -1.39445289e+00 -1.65617613e+00]\n",
      "  [-1.52475424e+00 -1.78880497e-01 -1.49917936e+00]\n",
      "  [-1.24882517e+00 -7.16266528e-02 -5.92467705e-01]\n",
      "  [-5.22944529e-03 -2.49415979e-01 -1.00213249e+00]\n",
      "  [-1.32712296e+00 -1.22757673e+00 -9.46457825e-01]\n",
      "  [-5.61356263e-02 -1.93626637e+00 -1.35483637e+00]\n",
      "  [-6.30516901e-01 -8.10184053e-01 -3.20970172e-01]]\n",
      "\n",
      " [[-1.17732804e+00 -1.83697065e-15 -1.28343557e+00]\n",
      "  [-1.94011213e+00 -7.21419010e-01 -1.35760941e+00]\n",
      "  [-4.40313873e-01 -1.02614727e+00 -1.60874929e+00]\n",
      "  [-1.90048309e+00 -5.11772925e-01 -1.76845292e+00]\n",
      "  [-4.54290891e-01 -1.35897954e+00 -1.60280156e+00]\n",
      "  [-2.90519987e-01 -1.22776759e+00 -1.72419902e-01]\n",
      "  [-2.96713409e-01 -6.00686913e-01 -1.21539497e+00]\n",
      "  [-2.94904278e-01 -3.00531374e-01 -1.44188461e+00]\n",
      "  [-7.23747116e-01 -1.35272168e+00 -9.41400372e-01]\n",
      "  [-6.69085084e-01 -8.41999573e-01 -1.22493548e+00]]\n",
      "\n",
      " [[-2.21116529e-01 -1.20913556e+00 -7.73310684e-01]\n",
      "  [-1.09825182e+00 -1.34295569e+00 -2.57722684e-01]\n",
      "  [-1.31338856e+00 -1.71610276e+00 -8.14056940e-01]\n",
      "  [-6.31345978e-01 -3.84741484e-01 -8.50998618e-01]\n",
      "  [-1.23951514e+00 -9.15286265e-01 -1.10987261e+00]\n",
      "  [-5.52204602e-01 -1.73358287e+00 -9.02250267e-01]\n",
      "  [-8.34987817e-01 -1.14834320e+00 -5.76512447e-01]\n",
      "  [-1.64711812e-02 -4.77619873e-01 -1.58987949e+00]\n",
      "  [-1.11220266e+00 -2.09994494e-01 -1.49448085e-01]\n",
      "  [-4.83229326e-01 -1.98250410e+00 -2.51149211e-01]]\n",
      "\n",
      " [[-4.06431332e-01 -7.90868275e-01 -1.00719417e+00]\n",
      "  [-1.49989541e+00 -7.03310978e-01 -1.57765396e+00]\n",
      "  [-6.42478374e-01 -6.68351238e-01 -5.01540029e-02]\n",
      "  [-1.15419985e+00 -1.28039256e-01 -9.79720488e-01]\n",
      "  [-8.90752254e-01 -1.63936416e+00 -7.80184146e-02]\n",
      "  [-1.26270506e+00 -4.12238862e-01 -1.64876399e+00]\n",
      "  [-7.47921055e-01 -1.08381076e+00 -7.92273638e-02]\n",
      "  [-3.39976027e-01 -1.92866794e+00 -7.32992932e-01]\n",
      "  [-1.12877372e+00 -1.26992605e-01 -1.01443844e+00]\n",
      "  [-4.04960355e-01 -8.63712171e-01 -1.95630631e+00]]\n",
      "\n",
      " [[-3.63176311e-02 -8.28195893e-01 -1.33546039e+00]\n",
      "  [-1.29944317e+00 -1.90923503e+00 -6.37621146e-01]\n",
      "  [-6.39394604e-01 -5.98256100e-01 -1.15101450e+00]\n",
      "  [-1.36962850e+00 -9.31162989e-02 -1.23409799e+00]\n",
      "  [-9.68464152e-01 -1.47826312e+00 -4.72520616e-01]\n",
      "  [-3.74429791e-02 -1.31928769e+00 -1.61153099e+00]\n",
      "  [-3.72021078e-01 -4.53638464e-01 -1.13577912e-01]\n",
      "  [-6.79829071e-01 -1.74643115e+00 -7.42555266e-01]\n",
      "  [-2.21000744e-01 -1.90884700e+00 -1.02716319e+00]\n",
      "  [-1.12434164e+00 -1.08830214e+00 -3.03915060e-01]]\n",
      "\n",
      " [[-5.65726816e-01 -1.52334693e+00 -1.59789047e+00]\n",
      "  [-6.77697709e-01 -1.22168216e+00 -6.05348079e-01]\n",
      "  [-7.68602918e-01 -2.11319248e-01 -1.53324061e-01]\n",
      "  [-1.25994072e+00 -1.48111049e+00 -1.30877177e+00]\n",
      "  [-1.44657784e-01 -1.25377259e+00 -4.85652603e-01]\n",
      "  [-1.08123624e-01 -1.78363988e+00 -5.05247453e-01]\n",
      "  [-1.50950078e+00 -1.20842378e+00 -6.34705525e-01]\n",
      "  [-1.18265549e+00 -1.96690763e+00 -9.12756546e-01]\n",
      "  [-9.68700519e-01 -1.62105394e-01 -1.26533270e+00]\n",
      "  [-1.56558079e+00 -6.74183273e-01 -9.67097864e-01]]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SailingEnv' object has no attribute 'current_step'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinal Q-table:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[39mprint\u001b[39m(q_table)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39;49mcurrent_step\u001b[39m}\u001b[39;00m\u001b[39m-th step of \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m-th ep., New Pos=\u001b[39m\u001b[39m{\u001b[39;00mnew_state\u001b[39m}\u001b[39;00m\u001b[39m, RWD=\u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m}\u001b[39;00m\u001b[39m, EP_REW=\u001b[39m\u001b[39m{\u001b[39;00mep_reward\u001b[39m}\u001b[39;00m\u001b[39m, Done=\u001b[39m\u001b[39m{\u001b[39;00mdone\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SailingEnv' object has no attribute 'current_step'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class SailingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=10, shape=(2,), dtype=np.int64)\n",
    "        self.wind_direction = np.array([0, 1])\n",
    "        self.position = np.array([0, 0])\n",
    "        self.rudder_angle = 0\n",
    "        self.target = np.array([8.0, 9.0])\n",
    "\n",
    "    def reset(self):\n",
    "        self.wind_direction = np.array([0, 1])\n",
    "        self.position = np.array([0, 0])\n",
    "        self.rudder_angle = 0\n",
    "        return self.position\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.rudder_angle = -1\n",
    "        elif action == 1:\n",
    "            self.rudder_angle = 0\n",
    "        else:\n",
    "            self.rudder_angle = 1\n",
    "        \n",
    "        wind_angle = np.arctan2(self.wind_direction[1], self.wind_direction[0])\n",
    "        boat_velocity = self.wind_direction - 5 * np.array([np.sin(wind_angle + np.deg2rad(self.rudder_angle)), np.cos(wind_angle + np.deg2rad(self.rudder_angle))])\n",
    "        relative_velocity = boat_velocity - self.wind_direction\n",
    "        boat_direction = np.arctan2(relative_velocity[1], relative_velocity[0])\n",
    "        heading_angle = boat_direction + np.deg2rad(self.rudder_angle)\n",
    "        self.position += boat_velocity.astype(int)\n",
    "        self.wind_direction = np.array([0, 1], dtype=float)\n",
    "\n",
    "        reward = np.cos(heading_angle - wind_angle)\n",
    "        done = self.position[0] >= 10 or self.position[1] >= 10 or self.position[0] < 0 or self.position[1] < 0\n",
    "        return self.position, reward, done, {}\n",
    "\n",
    "env = SailingEnv()\n",
    "\n",
    "# Q-learning algorithm\n",
    "num_episodes = 1000\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "#q_table will have 10X10X4 cells\n",
    "q_table = np.random.uniform(low=-2, high=0, size=([10,10] + [env.action_space.n]))\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPSILON=0.9\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.random() < epsilon:\n",
    "            # take a random action with probability epsilon\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # choose the action with the highest Q-value\n",
    "            action = np.argmax(q_table[observation[0], observation[1], :])\n",
    "        \n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        next_action = np.argmax(q_table[next_observation[0], next_observation[1], :])\n",
    "        \n",
    "        # update Q-value using Q-learning update rule\n",
    "        q_table[observation[0], observation[1], action] += alpha * (reward + gamma * q_table[next_observation[0], next_observation[1], next_action] - q_table[observation[0], observation[1], action])\n",
    "        observation = next_observation\n",
    "        episode_reward += reward\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Episode {i+1}/{num_episodes}, Reward: {episode_reward}\")\n",
    "        \n",
    "print(\"Training complete.\")\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific implementation, the agent randomly chooses actions using the env.action_space.sample() method. So, the output will differ each time you run the code. The important thing is to observe whether the total episode reward increases or not as the agent learns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Q-learning algorithm inside the main loop of your original code. The Q-table is initialized with zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rofos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2f5a264b2e5996d9fa0766bbb0f6d08929f71a45a794f4b8d0fed6d084d076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
