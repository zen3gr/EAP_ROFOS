{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95167c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gymnasium as gym\n",
    "import gym #for using stable baselines comparison\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "#env=gym.make(\"CartPole-v1\")\n",
    "#env = gym.make(\"MountainCar-v0\" , render_mode=\"human\")\n",
    "#env.reset()\n",
    "env.close()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = 2  # always go right!\n",
    "#     env.step(action)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space\n",
    "#a)first array is min value for position and velocity\n",
    "#b)second array is max value for position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    res=env.action_space.sample()\n",
    "    print(f\"{i}-th sampling gave {res} as the action\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99becfea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.high) #max-min position of distance and max-min value of velocity\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = 20 #poso tha \"temaxistei\" to observation kai action space\n",
    "DISCRETE_OS_SIZE = [dis, dis]\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "print(discrete_os_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e29d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "0.09*20 #is equal to [-1.2,0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1467e4",
   "metadata": {},
   "source": [
    "# Q-Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72775781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning settings\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 4*25001\n",
    "EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67848ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#greedy epsilon\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, qoing to be decayed\n",
    "#high epsilon -> agent prefers exploration\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2 #apo ayto to epeisodio kai meta to e = 0 = constant\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "END_EPSILON_DECAYING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon_old = []\n",
    "# epsilon=1\n",
    "# for ep in range(EPISODES):\n",
    "#     epsilon_old=np.append(epsilon_old, epsilon)\n",
    "#     if END_EPSILON_DECAYING >= ep >= START_EPSILON_DECAYING:\n",
    "            \n",
    "#             epsilon -= epsilon_decay_value\n",
    "# len(epsilon_old[epsilon_old < 0])  \n",
    "# #len(epsilon_old)\n",
    "# # #output = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.unique(epsilon_old)) == len(epsilon_old) # some episodes have same epsilon\n",
    "# #output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DISCRETE_OS_SIZE + [env.action_space.n])\n",
    "#20 possible x positions\n",
    "#20 possible velocity values\n",
    "#3 possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    if type(state) == tuple: #if epeidh to env.reset() epistrefei tuple, enw to env.step()[0] epistrefei array\n",
    "        #to new_state\n",
    "        state = state[0]\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    #return tuple(discrete_state.astype(int)) #older version?\n",
    "    return tuple(discrete_state.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_env_reset=env.reset()\n",
    "print(f\"state discrete: {get_discrete_state(same_env_reset)} \\nenv output :{same_env_reset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d344772",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=env.reset()\n",
    "q_table[get_discrete_state(random_state)] #epistrefei to Q-value gia ta 3 actions\n",
    "#o agent epilegei to megisto Q-value\n",
    "print(f\"q-table={q_table[get_discrete_state(random_state)]} \\nargmax = {np.argmax(q_table[get_discrete_state(random_state)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "for ep in range(EPISODES):\n",
    "    current_state = get_discrete_state(env.reset())\n",
    "    i=0 #agent's moves counter\n",
    "    done=False #it becomes true from env.step()\n",
    "    while not done:\n",
    "        #choose best action for current_state\n",
    "        action=np.argmax(q_table[current_state])\n",
    "        #after action, we have a new state and the reward for the action that moved us \"forward\"\n",
    "        new_state, reward, done, truncated = env.step(action)\n",
    "        new_state_x=new_state[0] #first value is x position #check x position for env.goal_position\n",
    "        new_state=get_discrete_state(new_state) #discretize our new state\n",
    "        #Qmax: apo ton mh ananewmeno pinaka, vriskw to megisto Q gia to \n",
    "        Q_max = np.max(q_table[new_state])\n",
    "        #Q: apo thn mhn ananewmeno pinaka, to Q to gia state kai to action poy phra\n",
    "        Q_current=q_table[current_state + (action,)]\n",
    "        #eq\n",
    "        new_q=(1 - LEARNING_RATE) * Q_current + LEARNING_RATE * (reward + DISCOUNT * Q_max)\n",
    "        #update Q table\n",
    "        q_table[current_state + (action,)] = new_q        \n",
    "        if new_state_x >= env.goal_position:\n",
    "            q_table[current_state + (action,)]=0\n",
    "            #print(f\"megalo X\")\n",
    "        if done==True and ep%5000==0:\n",
    "            print(f\"Done at {i}-th move durin {ep}-th episode.\")\n",
    "        else:\n",
    "            i=i+1\n",
    "        current_state=new_state\n",
    "\n",
    "#output\n",
    "# Done at 199-th move durin 0-th episode.\n",
    "# Done at 173-th move durin 5000-th episode.\n",
    "# Done at 170-th move durin 10000-th episode.\n",
    "# Done at 174-th move durin 15000-th episode.\n",
    "# Done at 110-th move durin 20000-th episode.\n",
    "# Done at 110-th move durin 25000-th episode.\n",
    "# Done at 110-th move durin 30000-th episode.\n",
    "# Done at 110-th move durin 35000-th episode.\n",
    "# Done at 172-th move durin 40000-th episode.\n",
    "# Done at 110-th move durin 45000-th episode.\n",
    "# Done at 110-th move durin 50000-th episode.\n",
    "# Done at 111-th move durin 55000-th episode.\n",
    "# Done at 172-th move durin 60000-th episode.\n",
    "# Done at 108-th move durin 65000-th episode.\n",
    "# Done at 108-th move durin 70000-th episode.\n",
    "# Done at 108-th move durin 75000-th episode.\n",
    "# Done at 115-th move durin 80000-th episode.\n",
    "# Done at 107-th move durin 85000-th episode.\n",
    "# Done at 107-th move durin 90000-th episode.\n",
    "# Done at 108-th move durin 95000-th episode.\n",
    "# Done at 109-th move durin 100000-th episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be823f",
   "metadata": {},
   "source": [
    "# Save trained q_table locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('trained_q_table'), q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0a12e",
   "metadata": {},
   "source": [
    "# Load trained q_table locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c37ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_table=np.load(os.path.join('trained_q_table.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q_table[q_table>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "# epsilon=1\n",
    "# for ep in range(EPISODES):\n",
    "#     current_state = get_discrete_state(env.reset())\n",
    "#     i=0 #agent's moves counter\n",
    "#     done=False #it becomes true from env.step()\n",
    "#     while not done:\n",
    "#         if np.random.random() > epsilon:\n",
    "#         #choose best action for current_state\n",
    "#             action=np.argmax(q_table[current_state])\n",
    "#         else:\n",
    "#             action=np.random.randint(0, env.action_space.n)\n",
    "#         #after action, we have a new state and the reward for the action that moved us \"forward\"\n",
    "#         new_state, reward, done, truncated , information = env.step(action)\n",
    "#         new_state_x=new_state[0] #first value is x position #check x position for env.goal_position\n",
    "#         new_state=get_discrete_state(new_state) #discretize our new state\n",
    "#         #Qmax: apo ton mh ananewmeno pinaka, vriskw to megisto Q gia to \n",
    "#         Q_max = np.max(q_table[new_state])\n",
    "#         #Q: apo thn mhn ananewmeno pinaka, to Q to gia state kai to action poy phra\n",
    "#         Q_current=q_table[current_state + (action,)]\n",
    "#         #eq\n",
    "#         new_q=(1 - LEARNING_RATE) * Q_current + LEARNING_RATE * (reward + DISCOUNT * Q_max)\n",
    "#         #update Q table\n",
    "#         q_table[current_state + (action,)] = new_q        \n",
    "#         if new_state_x >= env.goal_position:\n",
    "#             q_table[current_state + (action,)]=0\n",
    "#             #print(f\"megalo X\")\n",
    "#         if done==True and ep%5000==0:\n",
    "#             print(f\"Done with {i}-th move during {ep}-th episode. Epsilon={epsilon}\")\n",
    "#         else:\n",
    "#             i=i+1\n",
    "#         current_state=new_state\n",
    "#     if END_EPSILON_DECAYING >= ep >= START_EPSILON_DECAYING:\n",
    "#         epsilon -= epsilon_decay_value\n",
    "#         #print(f\"epsilon value = {epsilon}\")\n",
    "    \n",
    "# #output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ddde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal_position=env.goal_position\n",
    "# max_speed=env.observation_space.high[1]\n",
    "# min_speed=env.observation_space.low[1]\n",
    "# velocity=np.random.uniform(min_speed,max_speed)\n",
    "# goal_state=(goal_position,velocity)\n",
    "# get_discrete_state(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac326f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_position=env.observation_space.high[0]\n",
    "# min_position=env.observation_space.low[0]\n",
    "# print(f\"max position:{max_position}, min position:{min_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_discrete_state((max_position, velocity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_speed=env.observation_space.high[1]\n",
    "# min_speed=env.observation_space.low[1]\n",
    "# np.random.uniform(min_speed,max_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e1c28",
   "metadata": {},
   "source": [
    "# Manual training VS Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    current_state = get_discrete_state(env.reset())\n",
    "    i=0 #agent's moves counter\n",
    "    done=False #it becomes true from env.step()\n",
    "    while not done:\n",
    "        #choose best action for current_state\n",
    "        action=np.argmax(q_table[current_state])\n",
    "        #after action, we have a new state and the reward for the action that moved us \"forward\"\n",
    "        new_state, reward, done, truncated = env.step(action)\n",
    "        #new_state_x=new_state[0] #first value is x position #check x position for env.goal_position\n",
    "        new_state=get_discrete_state(new_state) #discretize our new state\n",
    "#         #Qmax: apo ton mh ananewmeno pinaka, vriskw to megisto Q gia to \n",
    "#         Q_max = np.max(q_table[new_state])\n",
    "#         #Q: apo thn mhn ananewmeno pinaka, to Q to gia state kai to action poy phra\n",
    "#         Q_current=q_table[current_state + (action,)]\n",
    "#         #eq\n",
    "#         new_q=(1 - LEARNING_RATE) * Q_current + LEARNING_RATE * (reward + DISCOUNT * Q_max)\n",
    "#         #update Q table\n",
    "#         q_table[current_state + (action,)] = new_q        \n",
    "#         if new_state_x >= env.goal_position:\n",
    "#             q_table[current_state + (action,)]=0\n",
    "#             #print(f\"megalo X\")\n",
    "#         if done==True and ep%5000==0:\n",
    "#             print(f\"Done at {i}-th move durin {ep}-th episode.\")\n",
    "#         else:\n",
    "        i=i+1\n",
    "        current_state=new_state\n",
    "    print(f\"Number of steps until Done: {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ec598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "dqn_model = DQN(\"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=1,\n",
    "            train_freq=16,\n",
    "            gradient_steps=8,\n",
    "            gamma=0.99,\n",
    "            exploration_fraction=0.2,\n",
    "            exploration_final_eps=0.07,\n",
    "            target_update_interval=600,\n",
    "            learning_starts=1000,\n",
    "            buffer_size=10000,\n",
    "            batch_size=128,\n",
    "            learning_rate=4e-3,\n",
    "            policy_kwargs=dict(net_arch=[256, 256]),\n",
    "            #tensorboard_log=tensorboard_log,\n",
    "            seed=2)\n",
    "     \n",
    "dqn_model.learn(total_timesteps=10000, log_interval=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.reset()\n",
    "done=False\n",
    "i=0\n",
    "while not done:\n",
    "    \n",
    "    action, wut = dqn_model.predict(obs)\n",
    "    new_state, reward, done, truncated = env.step(action)\n",
    "    obs=new_state\n",
    "    i=i+1\n",
    "print(f\"Number of steps until Done: {i+1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
