{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95167c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "#env = gym.make(\"MountainCar-v0\" , render_mode=\"human\")\n",
    "#env.reset()\n",
    "env.close()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = 2  # always go right!\n",
    "#     env.step(action)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space\n",
    "#a)first array is min value for position and velocity\n",
    "#b)second array is max value for position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    res=env.action_space.sample()\n",
    "    print(f\"{i}-th sampling gave {res} as the action\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99becfea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.high) #max-min position of distance and max-min value of velocity\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = 20 #poso tha \"temaxistei\" to observation kai action space\n",
    "DISCRETE_OS_SIZE = [dis, dis]\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "print(discrete_os_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e29d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "0.09*20 #is equal to [-1.2,0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de967040",
   "metadata": {},
   "source": [
    "# Q-Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c914ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning settings\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 25001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DISCRETE_OS_SIZE + [env.action_space.n])\n",
    "#20 possible x positions\n",
    "#20 possible velocity values\n",
    "#3 possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c201c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    if type(state) == tuple: #if epeidh to env.reset() epistrefei tuple, enw to env.step()[0] epistrefei array\n",
    "        #to new_state\n",
    "        state = state[0]\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    #return tuple(discrete_state.astype(int)) #older version?\n",
    "    return tuple(discrete_state.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_env_reset=env.reset()\n",
    "print(f\"state discrete: {get_discrete_state(same_env_reset)} \\nenv output :{same_env_reset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=env.reset()\n",
    "q_table[get_discrete_state(random_state)] #epistrefei to Q-value gia ta 3 actions\n",
    "#o agent epilegei to megisto Q-value\n",
    "print(f\"q-table={q_table[get_discrete_state(random_state)]} \\nargmax = {np.argmax(q_table[get_discrete_state(random_state)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "for ep in range(EPISODES):\n",
    "    current_state = get_discrete_state(env.reset())\n",
    "    i=0 #agent's moves counter\n",
    "    done=False #it becomes true from env.step()\n",
    "    while not done:\n",
    "        #choose best action for current_state\n",
    "        action=np.argmax(q_table[current_state])\n",
    "        #after action, we have a new state and the reward for the action that moved us \"forward\"\n",
    "        new_state, reward, done, trncated , information = env.step(action)\n",
    "        new_state=get_discrete_state(new_state) #discretize our new state\n",
    "        #Qmax: apo ton mh ananewmeno pinaka, vriskw to megisto Q gia to \n",
    "        Q_max = np.max(q_table[new_state])\n",
    "        #Q: apo thn mhn ananewmeno pinaka, to Q to gia state kai to action poy phra\n",
    "        Q_current=q_table[current_state + (action,)]\n",
    "        #eq\n",
    "        new_q=(1 - LEARNING_RATE) * Q_current + LEARNING_RATE * (reward + DISCOUNT * Q_max)\n",
    "        #update Q table\n",
    "        q_table[current_state + (action,)] = new_q\n",
    "        current_state=new_state\n",
    "        if done==True and ep%5000==0:\n",
    "            print(f\"Done at {i}-th move durin {ep}-th episode.\")\n",
    "        else:\n",
    "            i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_position=env.goal_position\n",
    "max_speed=env.observation_space.high[1]\n",
    "min_speed=env.observation_space.low[1]\n",
    "velocity=np.random.uniform(min_speed,max_speed)\n",
    "goal_state=(goal_position,velocity)\n",
    "get_discrete_state(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_position=env.observation_space.high[0]\n",
    "min_position=env.observation_space.low[0]\n",
    "print(f\"max position:{max_position}, min position:{min_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5632ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_discrete_state((max_position, velocity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_speed=env.observation_space.high[1]\n",
    "min_speed=env.observation_space.low[1]\n",
    "np.random.uniform(min_speed,max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "discrete_state=get_discrete_state(env.reset())\n",
    "done=False\n",
    "i=0\n",
    "while not done:\n",
    "\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    new_state, reward, done, _, _ = env.step(action)\n",
    "    #print(done)\n",
    "    new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "    #env.render()\n",
    "    #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "    # If simulation did not end yet after last step - update Q table\n",
    "    if not done:\n",
    "\n",
    "        # Maximum possible Q value in next step (for new state)\n",
    "        max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "        # Current Q value (for current state and performed action)\n",
    "        current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "        # And here's our equation for a new Q value for current state and action\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # Update Q table with new Q value\n",
    "        q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "#     # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "#     elif new_state[0] >= env.goal_position:\n",
    "#         #q_table[discrete_state + (action,)] = reward\n",
    "#         q_table[discrete_state + (action,)] = 0\n",
    "    print(f\"Done: {done}, i-th move:{i}\")\n",
    "    i=i+1\n",
    "    discrete_state = new_discrete_state\n",
    "\n",
    "\n",
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
