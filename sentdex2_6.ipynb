{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95167c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "#env = gym.make(\"MountainCar-v0\" , render_mode=\"human\")\n",
    "#env.reset()\n",
    "env.close()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = 2  # always go right!\n",
    "#     env.step(action)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space\n",
    "#a)first array is min value for position and velocity\n",
    "#b)second array is max value for position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    res=env.action_space.sample()\n",
    "    print(f\"{i}-th sampling gave {res} as the action\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99becfea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.high) #max-min position of distance and max-min value of velocity\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = 20 #poso tha \"temaxistei\" to observation kai action space\n",
    "DISCRETE_OS_SIZE = [dis, dis]\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "print(discrete_os_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e29d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "0.09*20 #is equal to [-1.2,0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1467e4",
   "metadata": {},
   "source": [
    "# Q-Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72775781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning settings\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 4*25001\n",
    "EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67848ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#greedy epsilon\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, qoing to be decayed\n",
    "#high epsilon -> agent prefers exploration\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2 #apo ayto to epeisodio kai meta to e = 0 = constant\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "END_EPSILON_DECAYING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon_old = []\n",
    "# epsilon=1\n",
    "# for ep in range(EPISODES):\n",
    "#     epsilon_old=np.append(epsilon_old, epsilon)\n",
    "#     if END_EPSILON_DECAYING >= ep >= START_EPSILON_DECAYING:\n",
    "            \n",
    "#             epsilon -= epsilon_decay_value\n",
    "# len(epsilon_old[epsilon_old < 0])  \n",
    "# #len(epsilon_old)\n",
    "# # #output = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.unique(epsilon_old)) == len(epsilon_old) # some episodes have same epsilon\n",
    "# #output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DISCRETE_OS_SIZE + [env.action_space.n])\n",
    "#20 possible x positions\n",
    "#20 possible velocity values\n",
    "#3 possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    if type(state) == tuple: #if epeidh to env.reset() epistrefei tuple, enw to env.step()[0] epistrefei array\n",
    "        #to new_state\n",
    "        state = state[0]\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    #return tuple(discrete_state.astype(int)) #older version?\n",
    "    return tuple(discrete_state.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fe0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_env_reset=env.reset()\n",
    "print(f\"state discrete: {get_discrete_state(same_env_reset)} \\nenv output :{same_env_reset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d344772",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=env.reset()\n",
    "q_table[get_discrete_state(random_state)] #epistrefei to Q-value gia ta 3 actions\n",
    "#o agent epilegei to megisto Q-value\n",
    "print(f\"q-table={q_table[get_discrete_state(random_state)]} \\nargmax = {np.argmax(q_table[get_discrete_state(random_state)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "for ep in range(EPISODES):\n",
    "    current_state = get_discrete_state(env.reset())\n",
    "    i=0 #agent's moves counter\n",
    "    done=False #it becomes true from env.step()\n",
    "    while not done:\n",
    "        #choose best action for current_state\n",
    "        action=np.argmax(q_table[current_state])\n",
    "        #after action, we have a new state and the reward for the action that moved us \"forward\"\n",
    "        new_state, reward, done, truncated , information = env.step(action)\n",
    "        new_state_x=new_state[0] #first value is x position #check x position for env.goal_position\n",
    "        new_state=get_discrete_state(new_state) #discretize our new state\n",
    "        #Qmax: apo ton mh ananewmeno pinaka, vriskw to megisto Q gia to \n",
    "        Q_max = np.max(q_table[new_state])\n",
    "        #Q: apo thn mhn ananewmeno pinaka, to Q to gia state kai to action poy phra\n",
    "        Q_current=q_table[current_state + (action,)]\n",
    "        #eq\n",
    "        new_q=(1 - LEARNING_RATE) * Q_current + LEARNING_RATE * (reward + DISCOUNT * Q_max)\n",
    "        #update Q table\n",
    "        q_table[current_state + (action,)] = new_q        \n",
    "        if new_state_x >= env.goal_position:\n",
    "            q_table[current_state + (action,)]=0\n",
    "            #print(f\"megalo X\")\n",
    "        if done==True and ep%5000==0:\n",
    "            print(f\"Done at {i}-th move durin {ep}-th episode.\")\n",
    "        else:\n",
    "            i=i+1\n",
    "        current_state=new_state\n",
    "\n",
    "#output\n",
    "# Done at 13516-th move durin 0-th episode.\n",
    "# Done at 151-th move durin 5000-th episode.\n",
    "# Done at 124-th move durin 10000-th episode.\n",
    "# Done at 107-th move durin 15000-th episode.\n",
    "# Done at 118-th move durin 20000-th episode.\n",
    "# Done at 107-th move durin 25000-th episode.\n",
    "# Done at 109-th move durin 30000-th episode.\n",
    "# Done at 110-th move durin 35000-th episode.\n",
    "# Done at 110-th move durin 40000-th episode.\n",
    "# Done at 110-th move durin 45000-th episode.\n",
    "# Done at 110-th move durin 50000-th episode.\n",
    "# Done at 107-th move durin 55000-th episode.\n",
    "# Done at 173-th move durin 60000-th episode.\n",
    "# Done at 109-th move durin 65000-th episode.\n",
    "# Done at 108-th move durin 70000-th episode.\n",
    "# Done at 108-th move durin 75000-th episode.\n",
    "# Done at 108-th move durin 80000-th episode.\n",
    "# Done at 175-th move durin 85000-th episode.\n",
    "# Done at 175-th move durin 90000-th episode.\n",
    "# Done at 108-th move durin 95000-th episode.\n",
    "# Done at 108-th move durin 100000-th episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q_table[q_table>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "epsilon=1\n",
    "for ep in range(EPISODES):\n",
    "    current_state = get_discrete_state(env.reset())\n",
    "    i=0 #agent's moves counter\n",
    "    done=False #it becomes true from env.step()\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "        #choose best action for current_state\n",
    "            action=np.argmax(q_table[current_state])\n",
    "        else:\n",
    "            action=np.random.randint(0, env.action_space.n)\n",
    "        #after action, we have a new state and the reward for the action that moved us \"forward\"\n",
    "        new_state, reward, done, truncated , information = env.step(action)\n",
    "        new_state_x=new_state[0] #first value is x position #check x position for env.goal_position\n",
    "        new_state=get_discrete_state(new_state) #discretize our new state\n",
    "        #Qmax: apo ton mh ananewmeno pinaka, vriskw to megisto Q gia to \n",
    "        Q_max = np.max(q_table[new_state])\n",
    "        #Q: apo thn mhn ananewmeno pinaka, to Q to gia state kai to action poy phra\n",
    "        Q_current=q_table[current_state + (action,)]\n",
    "        #eq\n",
    "        new_q=(1 - LEARNING_RATE) * Q_current + LEARNING_RATE * (reward + DISCOUNT * Q_max)\n",
    "        #update Q table\n",
    "        q_table[current_state + (action,)] = new_q        \n",
    "        if new_state_x >= env.goal_position:\n",
    "            q_table[current_state + (action,)]=0\n",
    "            #print(f\"megalo X\")\n",
    "        if done==True and ep%5000==0:\n",
    "            print(f\"Done with {i}-th move during {ep}-th episode. Epsilon={epsilon}\")\n",
    "        else:\n",
    "            i=i+1\n",
    "        current_state=new_state\n",
    "    if END_EPSILON_DECAYING >= ep >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "        #print(f\"epsilon value = {epsilon}\")\n",
    "    \n",
    "#output\n",
    "# Done with 16547-th move during 0-th episode. Epsilon=1\n",
    "# Done with 4826-th move during 5000-th episode. Epsilon=0.9000219995601054\n",
    "# Done with 1088-th move during 10000-th episode. Epsilon=0.8000239995202029\n",
    "# Done with 887-th move during 15000-th episode. Epsilon=0.7000259994803003\n",
    "# Done with 592-th move during 20000-th episode. Epsilon=0.6000279994403978\n",
    "# Done with 467-th move during 25000-th episode. Epsilon=0.5000299994004952\n",
    "# Done with 638-th move during 30000-th episode. Epsilon=0.40003199936059264\n",
    "# Done with 159-th move during 35000-th episode. Epsilon=0.3000339993206901\n",
    "# Done with 157-th move during 40000-th episode. Epsilon=0.20003599928071816\n",
    "# Done with 161-th move during 45000-th episode. Epsilon=0.10003799924069416\n",
    "# Done with 113-th move during 50000-th episode. Epsilon=3.9999200701827236e-05\n",
    "# Done with 135-th move during 55000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 151-th move during 60000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 144-th move during 65000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 148-th move during 70000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 153-th move during 75000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 142-th move during 80000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 105-th move during 85000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 144-th move during 90000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 154-th move during 95000-th episode. Epsilon=-1.9999599322172286e-05\n",
    "# Done with 138-th move during 100000-th episode. Epsilon=-1.9999599322172286e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ddde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_position=env.goal_position\n",
    "max_speed=env.observation_space.high[1]\n",
    "min_speed=env.observation_space.low[1]\n",
    "velocity=np.random.uniform(min_speed,max_speed)\n",
    "goal_state=(goal_position,velocity)\n",
    "get_discrete_state(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac326f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_position=env.observation_space.high[0]\n",
    "min_position=env.observation_space.low[0]\n",
    "print(f\"max position:{max_position}, min position:{min_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_discrete_state((max_position, velocity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_speed=env.observation_space.high[1]\n",
    "min_speed=env.observation_space.low[1]\n",
    "np.random.uniform(min_speed,max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "discrete_state=get_discrete_state(env.reset())\n",
    "done=False\n",
    "i=0\n",
    "while not done:\n",
    "\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    new_state, reward, done, _, _ = env.step(action)\n",
    "    #print(done)\n",
    "    new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "    #env.render()\n",
    "    #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "    # If simulation did not end yet after last step - update Q table\n",
    "    if not done:\n",
    "\n",
    "        # Maximum possible Q value in next step (for new state)\n",
    "        max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "        # Current Q value (for current state and performed action)\n",
    "        current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "        # And here's our equation for a new Q value for current state and action\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # Update Q table with new Q value\n",
    "        q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "\n",
    "#     # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "#     elif new_state[0] >= env.goal_position:\n",
    "#         #q_table[discrete_state + (action,)] = reward\n",
    "#         q_table[discrete_state + (action,)] = 0\n",
    "    print(f\"Done: {done}, i-th move:{i}\")\n",
    "    i=i+1\n",
    "    discrete_state = new_discrete_state\n",
    "\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25848e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
